{"cells":[{"cell_type":"markdown","metadata":{"id":"2HRT4nT91EQ5"},"source":["###INGESTION SERVICE NOTEBOOK\n","for CHATBOT"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"UEjAs-wTsUQ8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746073331656,"user_tz":-330,"elapsed":46533,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"174b9cf9-7e30-4ee5-9a7e-c5aa9e526e07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["###FIRST RUN\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeebPb150_47"},"outputs":[],"source":["###FIRST RUN\n","!pip install docx2txt torch transformers python-pptx Pillow pymupdf --quiet\n","!pip install llama-index llama-index-core llama-index-llms-groq llama-index-embeddings-huggingface --quiet\n","!pip install llama-index-readers-web --quiet\n","!pip install llama-index-llms-huggingface tokenizers --quiet\n","####GEMINI FOR ALL MULTIMODEL DATA TEXT + IMAGES\n","!pip install llama-index-vector-stores-qdrant llama-index-readers-file --quiet\n","!pip install llama-index-readers-file --quiet\n","###fast embedd\n","%pip install llama-index-embeddings-fastembed --quiet\n","%pip install fastembed --quiet\n","!pip install llama-index-embeddings-azure-openai\n","!pip install llama-index-llms-azure-openai"]},{"cell_type":"code","source":["### AZURE LIB\n","!pip install llama-index-embeddings-azure-openai\n","!pip install llama-index-llms-azure-openai\n","!pip install aioboto3 llama-index-readers-azstorage-blob"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQKgm5JQ0WaL","executionInfo":{"status":"ok","timestamp":1746001996296,"user_tz":-330,"elapsed":1331,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"90a83309-cac5-4680-f1b8-8a6fc0ad7ae4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing collected packages: jmespath, isodate, aioitertools, aiofiles, botocore, s3transfer, azure-storage-blob, aiobotocore, boto3, aioboto3, llama-index-readers-azstorage-blob\n","Successfully installed aioboto3-14.1.0 aiobotocore-2.21.1 aiofiles-24.1.0 aioitertools-0.12.0 azure-storage-blob-12.25.1 boto3-1.37.1 botocore-1.37.1 isodate-0.7.2 jmespath-1.0.1 llama-index-readers-azstorage-blob-0.3.0 s3transfer-0.11.3\n"]}]},{"cell_type":"code","source":["##AWS LIBRARIES\n","!pip install s3fs boto3 llama-index-embeddings-bedrock llama-index-llms-bedrock --quiet\n","!pip install llama-index-vector-stores-awsdocdb --quiet\n","!pip list > /content/requirements.txt"],"metadata":{"id":"UdFzO3AYlRb7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744999560410,"user_tz":-330,"elapsed":157920,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"3112e29e-d643-4980-f6bd-21c2675ea411"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Y7AaIboL1Ohx","executionInfo":{"status":"ok","timestamp":1746073537280,"user_tz":-330,"elapsed":27130,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}}},"outputs":[],"source":["###FIRST RUN\n","from llama_index.core import (\n","    VectorStoreIndex,\n","    SimpleDirectoryReader, Document,\n","    StorageContext,\n","    ServiceContext,\n","    load_index_from_storage\n",")\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.core.node_parser import SemanticSplitterNodeParser\n","from llama_index.llms.groq import Groq\n","# import os\n","# from dotenv import load_dotenv\n","# load_dotenv()\n","import warnings\n","warnings.filterwarnings('ignore')\n","###########SET configuration & KEYS & GLOBAL VARIABLES\n","# Load configuration from config.json file\n","import json\n","with open('/content/gdrive/MyDrive/genai/config_chatbot.json', 'r') as f:\n","    config = json.load(f)\n","#from google.colab import userdata\n","#GROQ_API_KEY = userdata.get('groq')\n","\n","GROQ_API_KEY = config['groq_api_key']\n","import os\n","GOOGLE_API_KEY = \"AIzaSyAuNDcCseSJqBAtOQ4e5mRqXWDHmbcduzs\"  # add your GOOGLE API key here\n","os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULhfgKAF1U-n"},"outputs":[],"source":["###READ DIRECTORY for all FILES and images ###FIRST RUN\n","### excluded_llm_metadata_keys MODIFY THIS USING CUSTOM METADATA EXTRACTOR\n","input_dirs= config['directories']\n","# Load documents from each directory individually\n","alldocuments = []\n","for dir in input_dirs:\n","    print(\"processing directory: \",dir)\n","    reader = SimpleDirectoryReader(filename_as_id=True,recursive = True,input_dir=dir, required_exts=[\".pdf\", \".docx\", \".txt\",\".xlsx\",\".csv\", \".pptx\",\".png\"] ,num_files_limit=100)\n","    alldocuments.extend(reader.load_data())\n","i=0\n","while i < len(alldocuments):\n","  print(alldocuments[i].metadata)\n","  i += 1\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qayNC1r3XVdX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746075136883,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"09ebfb0e-1c09-4e02-960e-e7bf88d1daf3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/genai/DEMO_DATA/Kyvos/Ebook-The-Ultimate-OLAP-Guide-1.pdf_part_0 The Ultimate\n","OLAP Guide  \n","EBOOKEBOOK {'page_label': '1', 'file_name': 'Ebook-The-Ultimate-OLAP-Guide-1.pdf', 'file_path': '/content/gdrive/MyDrive/genai/DEMO_DATA/Kyvos/Ebook-The-Ultimate-OLAP-Guide-1.pdf', 'file_type': 'application/pdf', 'file_size': 313338, 'creation_date': '2024-09-14', 'last_modified_date': '2024-09-14'}\n","<bound method BaseModel.schema of <class 'llama_index.core.schema.Document'>>\n","None\n","<bound method Node.get_content of Document(id_='/content/gdrive/MyDrive/genai/DEMO_DATA/Kyvos/Ebook-The-Ultimate-OLAP-Guide-1.pdf_part_0', embedding=None, metadata={'page_label': '1', 'file_name': 'Ebook-The-Ultimate-OLAP-Guide-1.pdf', 'file_path': '/content/gdrive/MyDrive/genai/DEMO_DATA/Kyvos/Ebook-The-Ultimate-OLAP-Guide-1.pdf', 'file_type': 'application/pdf', 'file_size': 313338, 'creation_date': '2024-09-14', 'last_modified_date': '2024-09-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The Ultimate\\nOLAP Guide  \\nEBOOKEBOOK', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')>\n","/content/gdrive/MyDrive/genai/DEMO_DATA/Kyvos/Ebook-The-Ultimate-OLAP-Guide-1.pdf_part_0\n"]}],"source":["# Get the attributes of the document\n","attributes = alldocuments[0]\n","# Print the attributes\n","print(attributes.id_,attributes.text,attributes.metadata)\n","print(attributes.schema)\n","print(attributes.embedding)\n","print(attributes.get_content)\n","print(attributes.doc_id)\n"]},{"cell_type":"markdown","metadata":{"id":"8NpqO0AG4khT"},"source":["LOAD MODELS AND SERVICE CONTEXT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DRZz9LS1bZm"},"outputs":[],"source":["######EMBEDDING MODEL###FIRST RUN\n","###########TEXT EMBEDDING USIGN HUGGING FACE\n","from llama_index.core import SimpleDirectoryReader, get_response_synthesizer\n","from llama_index.core import Settings\n","from llama_index.embeddings.fastembed import FastEmbedEmbedding\n","embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\" ,\n","                                   query_instruction=\"Embed customer questions for chatbot support.\",\n","                                   text_instruction=\"Embed documents related to impetus IT company provided solution, usecases implemented and software descriptions for customer inquiries.\"\n","                                   )\n","llm_groq_client = Groq(config['groq_model'], api_key=GROQ_API_KEY)\n","#Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")\n","Settings.embed_model= embed_model\n","Settings.llm=llm_groq_client\n","\n","####NODES WITH SemanticSplitterNodePadrser\n","from llama_index.core.node_parser import SemanticSplitterNodeParser\n","splitter = SemanticSplitterNodeParser(include_metadata=True,\n","    buffer_size=9, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",")\n","Settings.node_parser=splitter\n","trans=[splitter,Settings.embed_model,]\n","Settings.transformations = trans"]},{"cell_type":"code","source":["####MODEL LLM & EMBEDDING AWS\n","import os\n","from llama_index.embeddings.bedrock import BedrockEmbedding\n","import json\n","from llama_index.core import SimpleDirectoryReader, get_response_synthesizer\n","from llama_index.core import Settings\n","embed_model_aws = BedrockEmbedding(model_name=\"amazon.titan-embed-text-v2:0\",\n","    aws_access_key_id=\"AKIA4SYAMHZGI3QLGBV3\",\n","    aws_secret_access_key=\"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\",\n","    region_name=\"us-east-1\",\n","    system_prompt=\"Embedd the IT product document , so that user and do question and answer on that\",\n",")\n","supported_models = BedrockEmbedding.list_supported_models()\n","print(json.dumps(supported_models, indent=2))\n","###################################\n","###AWS LLM Components\n","#amazon.titan-embed-text-v2:0\n","#amazon.titan-embed-g1-text-02 < -- 1536 tried and tested\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zr1KwRFVqHV9","executionInfo":{"status":"ok","timestamp":1745002929534,"user_tz":-330,"elapsed":83,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"6439a739-ff90-418b-ff9a-0e3ede62b71f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"amazon\": [\n","    \"amazon.titan-embed-text-v1\",\n","    \"amazon.titan-embed-text-v2:0\",\n","    \"amazon.titan-embed-g1-text-02\"\n","  ],\n","  \"cohere\": [\n","    \"cohere.embed-english-v3\",\n","    \"cohere.embed-multilingual-v3\"\n","  ]\n","}\n"]}]},{"cell_type":"code","source":["#######AWS BASED\n","from llama_index.llms.bedrock import Bedrock\n","print(Bedrock.model_json_schema)\n","\n","llm_aws = Bedrock(\n","    model=\"amazon.titan-text-express-v1\",\n","     aws_access_key_id=\"AKIA4SYAMHZGI3QLGBV3\",\n","    aws_secret_access_key=\"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\",\n","    region_name=\"us-east-1\",\n","    temperature=0.5,\n","    system_prompt=\"You are sales assistant for impetus tech, helping agent to know about product based on provided knowledge only. Providing answer with citation of source\"\n",")\n","\n","Settings.embed_model= embed_model_aws\n","Settings.llm=llm_aws\n","\n","from llama_index.core.node_parser import SemanticSplitterNodeParser\n","splitter = SemanticSplitterNodeParser(include_metadata=True,\n","    buffer_size=50, breakpoint_percentile_threshold=75, embed_model=Settings.embed_model\n",")\n","Settings.node_parser=splitter\n","Settings.transformations = transformations=[splitter,Settings.embed_model,]\n"],"metadata":{"id":"Qgub6Gd_7OLO","executionInfo":{"status":"ok","timestamp":1745002934788,"user_tz":-330,"elapsed":2002,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"48fb75d0-aabe-4077-eafa-6fe7cd950dd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method BaseModel.model_json_schema of <class 'llama_index.llms.bedrock.base.Bedrock'>>\n"]}]},{"cell_type":"markdown","metadata":{"id":"mPlEh1MI8c4o"},"source":["GET AWS GLOBAL PEM FILE"]},{"cell_type":"markdown","metadata":{"id":"R_l_sFIV4h8M"},"source":["READ FILE ONE BY ONE"]},{"cell_type":"code","source":["# create the filesystem using s3fs\n","#s3://ingestionrepo/gathr/360-degree view of the customer for a large cable TV and telecom provider.pdf\n","\n","from s3fs import S3FileSystem\n","import boto3\n","\n","# Replace 'ingestionrepo' with your bucket name\n","bucket_name = \"ingestionrepo\"\n","region = \"us-east-1\"\n","print(f\"The region for bucket '{bucket_name}' is: us-east-1\")\n","\n","s3_fs1 = S3FileSystem(\n","    anon=False,\n","    key=\"AKIA4SYAMHZGI3QLGBV3\",\n","    secret=\"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\",\n","    client_kwargs={\"region_name\": region }\n",")\n","\n","reader = SimpleDirectoryReader(\n","    input_dir=\"ingestionrepo/test1\",\n","    fs=s3_fs1,\n","    recursive=False\n",")\n","\n","docs1 = await reader.aload_data()\n","print(f\"Loaded {len(docs1)} docs\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoW0YAY0cDqs","executionInfo":{"status":"ok","timestamp":1745003055791,"user_tz":-330,"elapsed":4261,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"edca1e0e-7115-4ea5-8405-99d6aade692a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The region for bucket 'ingestionrepo' is: us-east-1\n","Loaded 11 docs\n"]}]},{"cell_type":"code","source":["\n","import pymongo\n","from llama_index.vector_stores.awsdocdb import AWSDocDbVectorStore\n","mongo_uri ='mongodb://awsdocdb1:awsdocdb1@docdb-2025-02-06-rag.cluster-cr6mw6mag9mm.us-east-1.docdb.amazonaws.com:27017/?replicaSet=rs0&readPreference=secondaryPreferred&retryWrites=false'\n","mongodb_client = pymongo.MongoClient(mongo_uri)\n","\n","\n","try:\n","    # Connect to the cluster *without* specifying a database initially\n","    client = pymongo.MongoClient(mongo_uri)\n","\n","    db_name = \"testdb\"  # Choose a name for your database\n","\n","    db_list = client.list_database_names() # Get the list of databases\n","\n","    if db_name not in db_list:\n","        db = client[db_name] # Create the database\n","        print(f\"Database '{db_name}' created successfully.\")\n","    else:\n","        print(f\"Database '{db_name}' already exists.\")\n","\n","    # Now you can use the database in your LlamaIndex code\n","    # ... (Your LlamaIndex code using the 'db' object or db_name)\n","\n","except pymongo.errors.ConnectionFailure as e:\n","    print(f\"Error connecting to DocumentDB: {e}\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")\n","\n","print(mongodb_client.list_databases)\n","store = AWSDocDbVectorStore(mongodb_client)\n","storage_context = StorageContext.from_defaults(vector_store=store)\n","index = VectorStoreIndex.from_documents(\n","    docs, storage_context=storage_context\n",")"],"metadata":{"id":"-FldFseZAhmy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **AZURE BASED EMBEDDING AND LLM**"],"metadata":{"id":"GiiKCr01GfMQ"}},{"cell_type":"code","source":["#AZURE BASED EMBEDDING AND LLM USAGE\n","from llama_index.llms.azure_openai import AzureOpenAI\n","from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n","from llama_index.core import Settings\n","\n","api_key = \"7cONVniTkoyu5mVOY2FkC2Y58SEUvTAIarCH5vdhRU6bX7xfVyIXJQQJ99BCACYeBjFXJ3w3AAABACOGBabD\"\n","azure_endpoint = \"https://rag-accelerator-open-ai.openai.azure.com/\"\n","api_version = \"2024-02-15-preview\"\n","\n","llm = AzureOpenAI(\n","    model=\"gpt-4o-mini\",\n","    deployment_name=\"gpt-4o-mini\",\n","    api_key=api_key,\n","    azure_endpoint=azure_endpoint,\n","    api_version=api_version,\n",")\n","\n","# You need to deploy your own embedding model as well as your own chat completion model\n","embed_model = AzureOpenAIEmbedding(\n","    model=\"text-embedding-ada-002\",\n","    deployment_name=\"text-embedding-ada-002\",\n","    api_key=api_key,\n","    azure_endpoint=azure_endpoint,\n","    api_version=\"2023-05-15\",\n",")\n","Settings.llm = llm\n","Settings.embed_model = embed_model\n","\n","####NODES WITH SemanticSplitterNodePadrser\n","from llama_index.core.node_parser import SemanticSplitterNodeParser\n","splitter = SemanticSplitterNodeParser(include_metadata=True,\n","    buffer_size=50, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",")\n","Settings.node_parser=splitter\n","trans=[splitter,Settings.embed_model,]\n","Settings.transformations = trans"],"metadata":{"id":"OZit0Yg4GOdS","executionInfo":{"status":"ok","timestamp":1746076766168,"user_tz":-330,"elapsed":49,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"INCHMvTA4ewN"},"source":["##CREATE INDEX and STORAGE in QDRANT"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"dK3fZ40z15s8","executionInfo":{"status":"ok","timestamp":1746075248795,"user_tz":-330,"elapsed":9248,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}}},"outputs":[],"source":["# To connect to the same event-loop,\n","# allows async events to run on notebook\n","import nest_asyncio\n","nest_asyncio.apply()\n","from llama_index.vector_stores.qdrant import QdrantVectorStore\n","from qdrant_client import QdrantClient, AsyncQdrantClient\n","from qdrant_client.http.models import PointStruct\n","from qdrant_client import QdrantClient,AsyncQdrantClient , models\n","from qdrant_client.http import models as rest\n","from qdrant_client.models import (\n","    Distance,\n","    NamedSparseVector,\n","    NamedVector,\n","    SparseVector,\n","    PointStruct,\n","    SearchRequest,\n","    SparseIndexParams,\n","    SparseVectorParams,\n","    VectorParams,\n","    ScoredPoint,\n",")\n","###ONE HIT LOAD ALL DOC IN MEMORY VECTOR INDEX AND PERSIST IN DIRECTORY\n","#vector_index = VectorStoreIndex.from_documents(alldocuments, use_async=True, show_progress=True, service_context=service_context, node_parser=nodes)\n","#storage_context = StorageContext.from_defaults(persist_dir=\"/content/vector_storage1/storage_mini\")\n","#vector_index = VectorStoreIndex.from_documents(alldocuments, use_async=True, show_progress=True, embed_model=Settings.embed_model)\n","client =  QdrantClient(\n","    url=\"https://0bc37c56-d487-4e20-b8fd-8744c25e9ae3.us-east4-0.gcp.cloud.qdrant.io:6333\",\n","    api_key=\"7ev_7YpsgRdjybSDWu2aNPrYBQqjquPoEt9i3754AL3-rfOoApaK_Q\")\n","aclient =  AsyncQdrantClient(\n","    url=\"https://0bc37c56-d487-4e20-b8fd-8744c25e9ae3.us-east4-0.gcp.cloud.qdrant.io:6333\",\n","    api_key=\"7ev_7YpsgRdjybSDWu2aNPrYBQqjquPoEt9i3754AL3-rfOoApaK_Q\")\n","\n","dense_config = {\n","    \"text-dense\": rest.VectorParams(\n","        size=1024,  # Example vector size, adjust as needed\n","        distance=rest.Distance.COSINE  # Example distance metric\n","    )\n","}\n","text_vector_store = QdrantVectorStore(\n","    client=client ,\n","    enable_hybrid=True, batch_size=50,\n","    collection_name=\"azure_hybrid_collection\",\n","    aclient=aclient, use_async=True\n",")\n","storage_context_text = StorageContext.from_defaults(\n","    vector_store=text_vector_store)"]},{"cell_type":"markdown","source":["RUN INGESTION PIPELINE"],"metadata":{"id":"YGFQ_cL5RUmt"}},{"cell_type":"code","source":["from llama_index.core.ingestion import IngestionPipeline\n","pipeline = IngestionPipeline(name=\"RAG_QDRANT_TEXT_INGESTION\",\n","    transformations=Settings.transformations,\n","    vector_store=text_vector_store,\n",")\n","pipeline.disable_cache = True\n","#nodes = await pipeline.arun(documents=alldocuments, num_workers=4) : not working on T4\n","nodes = await pipeline.arun(documents=alldocuments,show_progress=True)\n","\n","len(nodes)"],"metadata":{"id":"Lup-8ovRDqk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llama_index.core import VectorStoreIndex\n","from IPython.display import display, Markdown\n","pipindex = VectorStoreIndex.from_vector_store(vector_store=text_vector_store)\n","qengine = pipindex.as_query_engine(use_async=True,\n","    similarity_top_k=5, sparse_top_k=5\n",")\n","\n","response= qengine.query(\"Can gathr helpful to any insurance company, give any implemented use case?\")\n","display(Markdown(str(response)))\n","\n","#query_engine = pipindex.as_query_engine()\n","#answer = query_engine.query(\"Can gathr helpful to any insurance company?\")\n","\n","#print(answer.get_formatted_sources())\n","#print(\"query was:\", \"Can gathr helpful to any insurance company?\")\n","#print(\"answer was:\", answer)\n","\n","#response=qengine.query(\"ON which day anshul got certified\")\n","\n","#query_engine = text_vector_store.as_query_engine(vector_store_query_mode=\"hybrid\")\n","#response=text_vector_store.aquery(\"Tell me about Data Platform Accelerator?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98},"id":"OPAA8IO_XTno","executionInfo":{"status":"ok","timestamp":1746076960144,"user_tz":-330,"elapsed":4158,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"57993924-8bca-46de-cf67-7be01e9f5766"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"Gathr can be beneficial for insurance companies by enabling real-time driver profiling and risk assessment through usage-based insurance programs. One implemented use case involves a leading auto insurance provider that utilized Gathr to ingest, transform, enrich, analyze, and store automotive telematics data in real-time. This allowed the company to build an end-to-end analytics application that offers dynamic, usage-based plans tailored to individual driver behavior, vehicle risks, and external factors like driving conditions and weather."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"LsQyOyfEtRI0"},"source":["SAVE INDEX"]},{"cell_type":"markdown","metadata":{"id":"mDU-UXM922CE"},"source":["NEW DOCUMENT UPSERTION ONE BY ONE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TaVsARyK2j5J"},"outputs":[],"source":["from llama_index.core.memory import ChatMemoryBuffer\n","memory = ChatMemoryBuffer.from_defaults(token_limit=9500)\n","\n","chat_engine = vector_index1.as_chat_engine(\n","    chat_mode=\"context\",\n","    memory=memory,\n","    llm=llm_groq_client,\n","    system_prompt=(\n","       \"You are a helpful AI Assistant in a Impetus Technologies Ltd and follow below instruction while answering.\\n\"\n","        \"Instructions: You have user query and context, If you find answer in provided all document, Synthesize information from multiple documents when relevant for answering.Combine information in a way that is clear,concise, and coherent.Avoid simply listing information from different sources.\"\n","          \"Prioritize the most relevant and important details from each document.Do not answer general question in any condition just deny.Answer only in provided format including citation.\\n\"\n","          \"Output Format (Answer in MarkDown Format): Answer: [Concise and direct answer to the question] \\n\"\n","           \"Source(s): [Detailed citation(s) with file path(s) for each piece of information used].\\n\"\n","          \"Cite sources with detailed information: filename, page number, section heading, slide title, etc.\\n\"\n","          \"Else if you are unable to find answer in all documents, reply this quoted line 'The given information is not provided in documents.\\n\"\n","          \"Do not answer from prior knowledge or internet in any condition.Reply normally to general user messages like greetings , appreciation.\\n\"\n","        \"Here is the context:\\n Use the previous chat history to interact and help the user.\\n\"\n","        \"{context_str}\"\n","\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5cvIwZF2l_w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733956814578,"user_tz":-330,"elapsed":81399,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"e75b5da2-c718-4a33-8018-4b23266e1240"},"outputs":[{"output_type":"stream","name":"stdout","text":["You: Hi\n","Assistant: Answer:  AI-generated documentation is a part of the Unity catalog feature set. It helps to solve the challenge of documentation maintenance, which is a difficult aspect of having a catalog.\n","Source(s): sales-presales/test/WIP - Pending Review - Accelerate Insights and Enhance Efficiency with AI-Generated Catalog Documentation.docx, 48:55 \n","\n","\n","\n","You: HI\n","Assistant: Hi there! 👋  How can I help you today? \n","\n","You: tell me about impetus data accelerator\n","Assistant: The provided documents do not contain information about \"Impetus Data Accelerator.\" \n","\n","The given information is not provided in documents. \n","\n","You: tell me about EKS ?\n","Assistant: The provided documents do not contain information about \"eks\". \n","\n","The given information is not provided in documents. \n","\n","You: what is kyvos ?\n","Assistant: Kyvos is an AI-powered semantic layer that accelerates analytics and AI initiatives. \n","\n","It establishes a universal semantic layer across an enterprise, standardizes data interpretation, and enables conversational interactions with data. Kyvos delivers high-speed analytics at any scale while significantly reducing analytics costs. \n","\n","Source(s): /content/gdrive/MyDrive/genai/DEMO_DATA/Kyvos/Kyvos-Retail-Chain-Transforms-Customer-Experiences-with-BI-Acceleration.pdf, About Kyvos section \n","\n","\n","\n","You: tell me any use case implemented on it\n","Assistant: The document  \"/content/gdrive/MyDrive/genai/DEMO_DATA/Kyvos/Kyvos-Retail-Chain-Transforms-Customer-Experiences-with-BI-Acceleration.pdf\" describes a use case of Kyvos implemented by a retail chain. \n","\n","They used Kyvos to accelerate their BI reporting and analytics, enabling them to:\n","\n","* **Improve customer experience:** Analyze customer behavior and preferences in real-time to personalize offers and recommendations.\n","* **Optimize inventory management:** Forecast demand and optimize stock levels to reduce waste and improve fulfillment rates.\n","* **Enhance operational efficiency:** Gain insights into store performance, identify bottlenecks, and streamline operations.\n","\n","Source(s): /content/gdrive/MyDrive/genai/DEMO_DATA/Kyvos/Kyvos-Retail-Chain-Transforms-Customer-Experiences-with-BI-Acceleration.pdf,  Retail Chain Use Case section \n","\n","\n","\n","\n","You: bye\n","Thank you for using the chatbot! Exiting...\n"]}],"source":["###as chatbot\n","import os\n","\n","# Initialize the chat history\n","chat_history = [\"\"]\n","\n","while True:\n","  # Get user input from the console\n","  user_input = input(\"You: \")\n","  user_input = user_input.lower()  # Convert to lowercase for case-insensitive exit\n","\n","  # Exit conditions\n","  if user_input in (\"bye\", \"exit\", \"thankyou\", \"good bye\"):\n","      print(\"Thank you for using the chatbot! Exiting...\")\n","      break\n","  elif user_input in (\"reset\",\"restart\"):\n","      print(\"Reset the chat history or session memory context\")\n","      chat_engine.reset()\n","      continue;\n","  # Append the user input to the chat history\n","  chat_history.append({\"role\": \"user\", \"content\": user_input})\n","\n","  response = chat_engine.chat(user_input)\n","\n","  # Print the response\n","  print(\"Assistant:\", response)\\\n","\n","    # Append the response to the chat history\n","  chat_history.append({\n","      \"role\": \"assistant\",\n","      \"content\": response\n","  })"]}],"metadata":{"colab":{"provenance":[{"file_id":"1J4DDiKIu1l02yTWBmWOfQU5uh2YazX7p","timestamp":1738817090366}],"gpuType":"T4","authorship_tag":"ABX9TyMheBZFo405mK5OPMWM47I8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}