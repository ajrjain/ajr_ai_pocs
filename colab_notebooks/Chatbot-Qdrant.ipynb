{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwg95qclrAfM7WvWJQNh5f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_poTsQ10WEKl"},"outputs":[],"source":["!pip install docx2txt torch transformers python-pptx Pillow\n","!pip install llama-index llama-index-llms-groq==0.1.3 groq==0.4.2 llama-index-embeddings-huggingface==0.2.0\n","\n","####GEMINI FOR ALL MULTIMODEL DATA TEXT + IMAGES\n","!pip install llama-index-embeddings-gemini\n","!pip install 'google-generativeai>=0.3.0' matplotlib\n","!pip install llama-index-multi-modal-llms-gemini\n","!pip install llama-index-vector-stores-qdrant\n","!pip install llama-index-llms-gemini\n","!pip install llama-index-readers-file pymupdf\n","\n","#######FOR IMAGES CLIP EMBEDDING\n","!pip install llama_hub\n","## VECTOR DB as CHROMADB\n","!pip install llama-index-vector-stores-qdrant\n","!pip install llama-index-multi-modal-llms-ollama\n","!pip install qdrant-client pdfminer.six python-docx python-pptx pandas"]},{"cell_type":"code","source":["from llama_index.core import (\n","    VectorStoreIndex,\n","    SimpleDirectoryReader, Document,\n","    StorageContext,\n","    ServiceContext,\n","    load_index_from_storage\n",")\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.core.node_parser import SemanticSplitterNodeParser\n","from llama_index.llms.groq import Groq\n","# import os\n","# from dotenv import load_dotenv\n","# load_dotenv()\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"5rwfQ98sWRD-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###########SET configuration & KEYS & GLOBAL VARIABLES\n","# Load configuration from config.json file\n","import json\n","with open('/content/config.json', 'r') as f:\n","    config = json.load(f)\n","\n","#from google.colab import userdata\n","#GROQ_API_KEY = userdata.get('groq')\n","\n","GROQ_API_KEY = config['groq_api_key']\n","import os\n","GOOGLE_API_KEY = \"AIzaSyAuNDcCseSJqBAtOQ4e5mRqXWDHmbcduzs\"  # add your GOOGLE API key here\n","os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"],"metadata":{"id":"pXMcCVdYWTCA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####LOAD IMAGES AND CREATE DOCUMENT <TUPLE --> having embedding of images using CLIP> LIST\n","#######MULTI MODEL INDEX\n","#Create and Use Qdrant Vector Store\n","#Set up two collections in Qdrant: one for text embeddings and one for image embeddings.\n","import qdrant_client\n","from qdrant_client.models import VectorParams, Distance\n","from transformers import AutoTokenizer, AutoModel\n","\n","# Define the schema for your collections\n","text_collection_name = \"text_collection\"\n","image_collection_name = \"image_collection\"\n","hf_text_model_name='sentence-transformers/all-MiniLM-L6-v2'\n","hf_image_model_name='openai/clip-vit-base-patch32'\n","hf_token=config['hf_token']\n","# Set up Qdrant client\n","client = qdrant_client.QdrantClient(path=\"/content/qdrant_storage1\",)\n","\n","#SAVE IMAGES EMBEDDING IN QDrANT\n","\n","from transformers import CLIPProcessor, CLIPModel\n","from PIL import Image\n","import torch\n","import os\n","import uuid\n","\n","from pdfminer.high_level import extract_text as extract_pdf_text\n","from docx import Document as DocxDocument\n","from pptx import Presentation\n","from llama_index.vector_stores.qdrant import QdrantVectorStore\n","from qdrant_client.http.models import PointStruct\n","\n","hf_model_name='openai/clip-vit-base-patch32'\n","clip_processor = CLIPProcessor.from_pretrained(hf_model_name, use_auth_token=config['hf_token'])\n","clip_model = CLIPModel.from_pretrained(hf_model_name, use_auth_token=config['hf_token'])\n","image_extensions = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\"}\n","text_extensions = {\".pdf\", \".ppt\", \".pptx\", \".txt\", \".rtf\",\".csv\",\".xlsx\",\".html\",\".md\",\".docx\",\".xls\"}\n","all_text_doclistWithembeddings=[]\n","all_image_doclistWithembeddings=[]\n","history_file = \"/content/load_history.json\"\n","text_tokenizer = AutoTokenizer.from_pretrained(hf_text_model_name, use_auth_token=hf_token)\n","text_model = AutoModel.from_pretrained(hf_text_model_name, use_auth_token=hf_token)\n","\n","# Initialize Qdrant vector stores\n","text_vector_store = QdrantVectorStore(\n","    client=client,\n","    collection_name=text_collection_name\n",")\n","\n","image_vector_store = QdrantVectorStore(\n","    client=client,\n","    collection_name=image_collection_name\n",")\n","\n","client.recreate_collection(\n","    collection_name=text_collection_name,\n","    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",")\n","client.recreate_collection(\n","    collection_name=image_collection_name,\n","    vectors_config=VectorParams(size=512, distance=Distance.COSINE)\n",")"],"metadata":{"id":"gd_6GLYRWWMn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def load_history():\n","        if os.path.exists(history_file):\n","            with open(history_file, 'r') as f:\n","                return json.load(f)\n","        return {}\n","\n","def save_history():\n","        with open(history_file, 'w') as f:\n","            json.dump(file_history, f)\n","\n","def processimage_document(file_path):\n","        extension = os.path.splitext(file_path)[1].lower()\n","        if extension in image_extensions:\n","          if should_process(file_path):\n","            return embed_image(file_path)\n","\n","def embed_image(file_path):\n","        # Open the image file\n","        image = Image.open(file_path)\n","        # Process the image using CLIP\n","        inputs = clip_processor(images=image, return_tensors=\"pt\")\n","        with torch.no_grad():\n","            embeddings = clip_model.get_image_features(**inputs)\n","        # Create a Document object\n","        print(\"reading \", file_path)\n","        document = SimpleDirectoryReader(input_files=[file_path]).load_data()\n","        # Attach embeddings to the document\n","        document[0].metadata['embeddings'] = embeddings\n","        document[0].metadata['id'] = str(uuid.uuid4())\n","         # Prepare Qdrant PointStruct\n","        point = PointStruct(\n","            id=document[0].metadata['id'],\n","            vector=document[0].metadata['embeddings'][0].tolist(),\n","            payload={\"file_path\": file_path}\n","        )\n","        client.upsert(\n","            collection_name=image_collection_name,\n","            points=[point]\n","        )\n","        print(\"Embedding saved for \", file_path)\n","        return document[0]\n","\n","def extract_text_content( file_path):\n","        extension = os.path.splitext(file_path)[1].lower()\n","        text, images = \"\", []\n","        if extension == \".pdf\":\n","            text, images = extract_pdf_content(file_path)\n","        elif extension == \".docx\":\n","            text, images = extract_docx_content(file_path)\n","        elif extension == \".pptx\":\n","            text, images = extract_pptx_content(file_path)\n","        elif extension == \".xlsx\":\n","            text = extract_xlsx_content(file_path)\n","        print(\"reading \", file_path)\n","        document = SimpleDirectoryReader(input_files=[file_path]).load_data()\n","        print(\"prepared doc for \", file_path)\n","        type(document)\n","        len(document)\n","\n","        document[0].metadata['embeddings'] = embed_text(text)\n","        document[0].metadata['id'] = str(uuid.uuid4())\n","\n","        # Prepare Qdrant PointStruct\n","        point = PointStruct(\n","            id=document[0].metadata['id'],\n","            vector=document[0].metadata['embeddings'][0].tolist(),\n","            payload={\"file_path\": file_path, \"text\": text}\n","        )\n","        client.upsert(\n","            collection_name=text_collection_name,\n","            points=[point]\n","        )\n","        print(\"Embedding with 384 saved for \",file_path)\n","        return document[0]\n","\n","def extract_pdf_content(file_path):\n","        text = extract_pdf_text(file_path)\n","        images = extract_images_from_pdf(file_path)\n","        return text, images\n","\n","def extract_docx_content( file_path):\n","        doc = DocxDocument(file_path)\n","        text = \"\\n\".join([para.text for para in doc.paragraphs])\n","        images = extract_images_from_docx(file_path)\n","        return text, images\n","\n","def extract_pptx_content(file_path):\n","        prs = Presentation(file_path)\n","        text = \"\\n\".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, \"text\")])\n","        images = extract_images_from_pptx(file_path)\n","        return text, images\n","\n","def extract_xlsx_content(file_path):\n","        df = pd.read_excel(file_path)\n","        return df.to_string()\n","\n","def extract_images_from_pdf(file_path):\n","        # Implement image extraction from PDF\n","        return []\n","\n","def extract_images_from_docx(file_path):\n","        # Implement image extraction from DOCX\n","        return []\n","\n","def extract_images_from_pptx(file_path):\n","        # Implement image extraction from PPTX\n","        return []\n","\n","def embed_text(text):\n","        inputs = text_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","        with torch.no_grad():\n","            outputs = text_model(**inputs)\n","            embeddings = outputs.last_hidden_state.mean(dim=1)\n","        return embeddings.numpy()\n","\n","def should_process(file_path):\n","        if not os.path.exists(file_path):\n","            return False\n","        file_mod_time = os.path.getmtime(file_path)\n","        if file_path not in file_history or file_history[file_path] < file_mod_time:\n","            file_history[file_path] = file_mod_time\n","            return True\n","        return False\n","\n","def parse_document( file_path):\n","      if  os.path.isfile(filepath):\n","        extension = os.path.splitext(file_path)[1].lower()\n","        if extension in text_extensions:\n","            return extract_text_content(file_path)\n"],"metadata":{"id":"m8XL2y92WazL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###EMBEDD IMAGES\n","## READ HISTORY TO AVOID PROCESSING SAME FILE AGAIN\n","file_history = load_history()\n","\n","for filename in os.listdir(config['directories_images']):\n","            filepath = os.path.join(config['directories_images'], filename)\n","            if not os.path.isfile(filepath):\n","                continue  # Skip non-file items\n","            getimagedocumentWIthEmbedding = processimage_document(filepath)\n","            all_image_doclistWithembeddings.append(getimagedocumentWIthEmbedding)\n","\n","##RECORD FILES ALREADY READ AND PROCESSED\n","save_history()\n","len(all_image_doclistWithembeddings)"],"metadata":{"id":"j-9oM9DfWdKQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## READ HISTORY TO AVOID PROCESSING SAME FILE AGAIN\n","file_history = load_history()\n","####PROCESSS OTHER THAN IMAGES FILES\n","all_text_doclistWithembeddings=[]\n","for dirfullpath in config['directories']:\n","            if not os.path.isdir(dirfullpath):\n","                print(\"Not a directory\", dirfullpath)\n","                continue  # Skip non-dir items\n","            for filename in os.listdir(dirfullpath):\n","              extension = os.path.splitext(filename)[1].lower()\n","              if extension in text_extensions:\n","                if should_process(dirfullpath+\"/\"+filename):\n","                  print(\"processing file\", filename)\n","                  all_text_doclistWithembeddings.append(parse_document(dirfullpath+\"/\"+filename))\n","\n","save_history()\n","len(all_text_doclistWithembeddings)"],"metadata":{"id":"JgHgE99wWd0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### WITH EMBEDDINGS PREPARE NODES\n","from llama_index.core import Settings\n","from llama_index.core.node_parser import SemanticSplitterNodeParser\n","\n","splitter = SemanticSplitterNodeParser(\n","    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",")\n","nodes_with_embeddings = splitter.get_nodes_from_documents(all_text_doclistWithembeddings)\n","nodes_img_embedded = splitter.get_nodes_from_documents(all_image_doclistWithembeddings)"],"metadata":{"id":"peH916uRWoB-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llama_index.core.postprocessor import SentenceTransformerRerank\n","# We choose a model with relatively high speed and decent accuracy.\n","\n","#Integrate LlamaIndex for Multi-Model Vector Index\n","#Integrate LlamaIndex with the Qdrant vector store to create a multi-model vector index.\n","from llama_index.core import (\n","    VectorStoreIndex,\n","    SimpleDirectoryReader,\n","    StorageContext,\n",")\n","from llama_index.vector_stores.qdrant import QdrantVectorStore\n","\n","# construct vector store and customize storage context\n","storage_context_images = StorageContext.from_defaults(\n","    vector_store=image_vector_store)\n","\n","storage_context_text = StorageContext.from_defaults(\n","    vector_store=text_vector_store)\n","\n","service_context_emb_text = ServiceContext.from_defaults(embed_model=HuggingFaceEmbedding(model_name=hf_text_model_name), llm=llm_groq_client)\n","service_context_emb_image = ServiceContext.from_defaults(embed_model=HuggingFaceEmbedding(model_name=hf_image_model_name), llm=clip_model)\n","\n","\n","# Initialize the query engine\n","index_path = \"/content/embedded_path_storage/\"\n","\n","#emd_index = VectorStoreIndex.from_documents(all_text_doclistWithembeddings,chunk_size=4096,nodes_parsers=nodes_with_embeddings)\n","\n","# build index\n","emd_text_index = VectorStoreIndex(nodes_with_embeddings, service_context=service_context_emb_text)\n","emd_images_index = VectorStoreIndex(nodes_img_embedded,service_context=service_context)\n","query_engine = emd_text_index.get_query_engine()\n","\n","response = query_engine.query(\"describe the parquet header components as per documents provided only?\")\n","print(response)"],"metadata":{"id":"NM2TJnodWsrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###READER\n","%pip install llama-index-readers-qdrant"],"metadata":{"id":"C9gzRNXcYsYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llama_index.readers.qdrant import QdrantReader\n","\n","from qdrant_client import QdrantClient\n","from qdrant_client.http.models import ScoredPoint\n","\n","try:\n","            client.close()\n","            print(\"Qdrant client closed successfully.\")\n","except Exception as e:\n","            print(f\"Error closing Qdrant client: {e}\")\n","# Initialize Qdrant client pointing to the local storage\n","# Define QdrantReader using the correct initialization\n","reader = QdrantReader(path=\"/content/qdrant_storage1\")\n","# NOTE: Required args are collection_name, query_vector.\n","# See the Python client: https://github.com/qdrant/qdrant_client\n","# for more details.\n","query_vector = [0] * 384\n","\n","# Load data from Qdrant\n","documents = reader.load_data(\n","    collection_name=\"text_collection\",\n","    query_vector=query_vector,\n","    limit=5\n",")\n","\n","# Print loaded documents\n","for doc in documents:\n","    print(doc)"],"metadata":{"id":"79U90YzPYuy1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pptx\n","print(f\"pptx version: {pptx.__version__}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQ2BtEr2WgO1","executionInfo":{"status":"ok","timestamp":1720173937428,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"63cb5804-171b-4860-df07-58ca83a6b973"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["pptx version: 0.6.23\n"]}]},{"cell_type":"code","source":["import os\n","from docx import Document\n","from pptx import Presentation\n","import fitz  # PyMuPDF library for PDFs\n","from bs4 import BeautifulSoup  # For HTML parsing\n","\n","\n","def extract_images(filepath, output_folder):\n","  \"\"\"\n","  Extracts images from various document formats and saves them with filename_descriptor.png format.\n","\n","  Args:\n","      filepath (str): Path to the document file.\n","      output_folder (str): Path to the folder where images will be saved.\n","  \"\"\"\n","  filename, ext = os.path.splitext(os.path.basename(filepath))\n","  os.makedirs(output_folder, exist_ok=True)\n","\n","  if ext == '.docx':\n","    doc = Document(filepath)\n","    for i, image in enumerate(doc.inline_shapes.graphics):\n","      image.image.save(os.path.join(output_folder, f\"{filename}_{i+1}.png\"))\n","\n","  elif ext == '.pptx':\n","    prs = Presentation(filepath)\n","    for slide_idx, slide in enumerate(prs.slides):\n","      for i, pic in enumerate(slide.shapes.pictures):\n","        if isinstance(shape, picture_type):\n","          pic.image.save(os.path.join(output_folder, f\"{filename}_{slide_idx+1}_{i+1}.png\"))\n","\n","  elif ext == '.pdf':\n","    doc = fitz.open(filepath)\n","    for page_idx in range(len(doc)):\n","      page = doc.load_page(page_idx)\n","      images = page.get_images()\n","      for i, image in enumerate(images):\n","        # Extract image data and save as PNG\n","        if len(image) == 6:\n","          x0, y0, x1, y1, link, pix = image  # Unpack only if length is 6\n","          if pix:\n","            image_data = pix.as_png_string()\n","            with open(os.path.join(output_folder, f\"{filename}_{page_idx+1}_{i+1}.png\"), 'wb') as f:\n","              f.write(image_data)\n","        else:\n","          print(f\"Unexpected data format for image: {image}\")\n","  # Handle unsupported formats (xls, md, webp)\n","  elif ext in ('.xls', '.xlsx', '.md', '.webp'):\n","    print(f\"Image extraction from '{ext}' format not directly supported yet.\")\n","\n","  else:\n","    print(f\"File format '{ext}' not supported.\")\n","\n","# Example usage\n","output_folder = \"/content/extracted_images\"\n","for filename in os.listdir(\"/content/test_mix\"):\n","            filepath = os.path.join(\"/content/test_mix\", filename)\n","            print(\"processing \", filename)\n","            extract_images(filepath, output_folder)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":662},"id":"usS7EQAWCREB","executionInfo":{"status":"error","timestamp":1720173442722,"user_tz":-330,"elapsed":456,"user":{"displayName":"Anshul Jain","userId":"02932772474875731417"}},"outputId":"919af5dc-28e4-43c6-9225-dcc58299342f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["processing  UserManual_JointDeclaration_member.pdf\n","Unexpected data format for image: (13, 0, 225, 225, 8, 'DeviceRGB', '', 'Image13', 'FlateDecode')\n","Unexpected data format for image: (38, 0, 1426, 640, 8, 'DeviceRGB', '', 'Image38', 'FlateDecode')\n","Unexpected data format for image: (39, 0, 1430, 545, 8, 'DeviceRGB', '', 'Image39', 'FlateDecode')\n","Unexpected data format for image: (40, 0, 1332, 359, 8, 'DeviceRGB', '', 'Image40', 'FlateDecode')\n","Unexpected data format for image: (43, 0, 1426, 1812, 8, 'DeviceRGB', '', 'Image43', 'FlateDecode')\n","Unexpected data format for image: (46, 0, 1431, 1827, 8, 'DeviceRGB', '', 'Image46', 'FlateDecode')\n","Unexpected data format for image: (49, 0, 1430, 1252, 8, 'DeviceRGB', '', 'Image49', 'FlateDecode')\n","Unexpected data format for image: (62, 0, 1363, 1097, 8, 'DeviceRGB', '', 'Image62', 'FlateDecode')\n","Unexpected data format for image: (65, 0, 1426, 1205, 8, 'DeviceRGB', '', 'Image65', 'FlateDecode')\n","Unexpected data format for image: (66, 0, 1426, 718, 8, 'DeviceRGB', '', 'Image66', 'DCTDecode')\n","Unexpected data format for image: (69, 0, 1428, 1208, 8, 'DeviceRGB', '', 'Image69', 'FlateDecode')\n","Unexpected data format for image: (72, 0, 1426, 1289, 8, 'DeviceRGB', '', 'Image72', 'FlateDecode')\n","Unexpected data format for image: (73, 0, 1213, 610, 8, 'DeviceRGB', '', 'Image73', 'FlateDecode')\n","Unexpected data format for image: (77, 0, 1428, 718, 8, 'DeviceRGB', '', 'Image77', 'FlateDecode')\n","Unexpected data format for image: (78, 0, 1328, 462, 8, 'DeviceRGB', '', 'Image78', 'FlateDecode')\n","Unexpected data format for image: (81, 0, 1329, 373, 8, 'DeviceRGB', '', 'Image81', 'FlateDecode')\n","Unexpected data format for image: (82, 0, 1460, 415, 8, 'DeviceRGB', '', 'Image82', 'FlateDecode')\n","Unexpected data format for image: (83, 0, 1463, 310, 8, 'DeviceRGB', '', 'Image83', 'FlateDecode')\n","Unexpected data format for image: (87, 0, 1424, 713, 8, 'DeviceRGB', '', 'Image87', 'FlateDecode')\n","processing  ML_KT_DS_Session1.pptx\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'SlideShapes' object has no attribute 'pictures'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-a3593fbc176e>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/test_mix\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processing \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mextract_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-a3593fbc176e>\u001b[0m in \u001b[0;36mextract_images\u001b[0;34m(filepath, output_folder)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPresentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mslide_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslide\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslide\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpictures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpicture_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{filename}_{slide_idx+1}_{i+1}.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'SlideShapes' object has no attribute 'pictures'"]}]},{"cell_type":"code","source":["###CODE TO SHOW IMAGES IN NOTEBOOK\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import os\n","\n","\n","def plot_images(image_paths):\n","    images_shown = 0\n","    plt.figure(figsize=(16, 9))\n","    for img_path in image_paths:\n","        if os.path.isfile(img_path):\n","            image = Image.open(img_path)\n","\n","            plt.subplot(2, 3, images_shown + 1)\n","            plt.imshow(image)\n","            plt.xticks([])\n","            plt.yticks([])\n","\n","            images_shown += 1\n","            if images_shown >= 9:\n","                break\n","# show sources\n","from llama_index.core.response.notebook_utils import display_source_node\n","\n","for text_node in response.metadata[\"text_nodes\"]:\n","    display_source_node(text_node, source_length=200)\n","plot_images(\n","    [n.metadata[\"file_path\"] for n in response.metadata[\"image_nodes\"]]\n",")"],"metadata":{"id":"0Qelc6O-I1rv"},"execution_count":null,"outputs":[]}]}